{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Helper function to normalize text\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) if text is not None else \"\"\n",
    "    return text.lower().strip().replace(\"_\", \"\").replace(\"?\", \"\").replace(\"\\\\\", \"\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load and preprocess data\n",
    "data_clean = pd.read_excel(\"/content/mat_na_0510_AI_AT_clean_train.xlsx\").dropna()\n",
    "data_clean['product_name'] = data_clean['product_name'].apply(normalize_text)\n",
    "data_clean['brand_clean'] = data_clean['brand_clean'].apply(normalize_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = torch.tensor(label_encoder.fit_transform(data_clean['brand_clean'])).long()\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "phobert_model = AutoModel.from_pretrained('intfloat/e5-large-v2').to(device)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_clean['product_name'], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to encode text data\n",
    "def encode_texts(texts, tokenizer):\n",
    "    encoded = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return encoded['input_ids'].to(device), encoded['attention_mask'].to(device)\n",
    "\n",
    "# Encode training and test data\n",
    "X_train_ids, X_train_mask = encode_texts(X_train.tolist(), tokenizer)\n",
    "X_test_ids, X_test_mask = encode_texts(X_test.tolist(), tokenizer)\n",
    "\n",
    "# Custom classifier model definition\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, phobert_model, num_labels):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.phobert = phobert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_labels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CustomClassifier(phobert_model, num_labels).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "best_accuracy = 0\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc_train = 0\n",
    "    for input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        total_acc_train += (predictions == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = total_acc_train / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            total_acc += (predictions == labels).sum().item()\n",
    "    \n",
    "    accuracy = total_acc / len(test_dataset)\n",
    "    print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "        print(\"Model and label encoder saved.\")\n",
    "\n",
    "# Function to make predictions on a new dataset\n",
    "def predict(file_path, file_output):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['product_name'] = df['product_name'].apply(normalize_text)\n",
    "    input_ids, attention_mask = encode_texts(df['product_name'].tolist(), tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(input_ids), 16)):\n",
    "            outputs = model(input_ids[i:i+16], attention_mask[i:i+16])\n",
    "            batch_scores = torch.softmax(outputs, dim=-1)\n",
    "            batch_predictions = torch.argmax(batch_scores, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            scores.extend(batch_scores.max(dim=-1).values.cpu().numpy())\n",
    "\n",
    "    df['predicted_label'] = label_encoder.inverse_transform(predictions)\n",
    "    df['scores'] = scores\n",
    "    df.to_excel(file_output, index=False)\n",
    "    print(f\"Prediction results saved to {file_output}\")\n",
    "\n",
    "# Call predict function\n",
    "predict(\"/content/mat_na_0510_AI_AT_clean.xlsx\", \"/content/mat_na_0510_AI_AT_clean_AI_txt.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
