{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "\n",
    "## Load model\n",
    "model_alibaba = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
    "model_name_or_path = 'Alibaba-NLP/gte-multilingual-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "df = pd.read_excel(r\"/content/mask_train.xlsx\")\n",
    "\n",
    "\n",
    "## Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "## Function\n",
    "# def encode(text):\n",
    "#     result = tokenizer(input_texts, max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "#     return result['input_ids'], result['attention_mask']\n",
    "\n",
    "def encode(text_list):\n",
    "    result = tokenizer(text_list, max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "    return result['input_ids'], result['attention_mask']\n",
    "\n",
    "\n",
    "\n",
    "def normalized(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "\n",
    "df['product_name'] = df['product_name'].apply(normalized)\n",
    "df['brand_clean'] = df['brand_clean'].apply(normalized)\n",
    "\n",
    "\n",
    "## Label\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y_encode = label_encoder.fit_transform(df['brand_clean'])\n",
    "num_classes = len(df['brand_clean'].unique())\n",
    "y_encode = torch.tensor(y_encode).long()\n",
    "\n",
    "\n",
    "## Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['product_name'], y_encode, test_size=0.2, random_state=42)\n",
    "\n",
    "## Encode cho tập huấn luyện và kiểm tra\n",
    "train_input_ids, train_attention_mask = encode(X_train.to_list())\n",
    "test_input_ids, test_attention_mask = encode(X_test.to_list())\n",
    "\n",
    "train_input_ids, train_attention_mask = encode(X_train.to_list())\n",
    "test_input_ids, test_attention_mask = encode(X_test.to_list())\n",
    "\n",
    "## Tạo TensorDataset từ tensor đã mã hóa và nhãn tương ứng\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, y_train)\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, y_test)\n",
    "\n",
    "## DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "## Custom model\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,model,num_classes):\n",
    "        super(Custom_Model,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state[:, 0, :]  # BERT embeddings\n",
    "        x = self.relu(self.fc1(last_hidden_state))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc3(x)  # Logits are the raw predictions\n",
    "        return logits  # Ensure that logits are returned\n",
    "\n",
    "## Define model\n",
    "model = Custom_Model(model = model_alibaba,num_classes = num_classes).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 3e-5)\n",
    "\n",
    "\n",
    "best_accuracy = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc_train = 0\n",
    "    for batch in tqdm(train_dataloader,desc = f\"Epoch{epoch +1 }/{EPOCHS}\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids,attention_mask)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs,dim = -1)\n",
    "        total_acc_train += (predictions == labels).sum().item()\n",
    "    train_accuracy = total_acc_train / len(train_data)\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average loss: {average_loss:.4f}\")\n",
    "    print(f\"Train accuracy: {train_accuracy}\")\n",
    "\n",
    "    ##Evaluate\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader,desc = f\"Epoch{epoch +1 }/{EPOCHS}\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids,attention_mask)\n",
    "            predictions = torch.argmax(outputs,dim = -1)\n",
    "            total_acc += (predictions == labels).sum().item()\n",
    "        accuracy = total_acc / len(test_data)\n",
    "        print(f\"Test accuracy: {accuracy}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            joblib.dump(label_encoder, 'big_label_encoder.pkl')\n",
    "            torch.save(model.state_dict(),'best_model.pth')\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict files\n",
    "\n",
    "## Load model\n",
    "model_alibaba = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
    "model_name_or_path = 'Alibaba-NLP/gte-multilingual-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "## Down load encoder\n",
    "label_encoder = joblib.load('/content/big_label_encoder.pkl')\n",
    "num_labels = len(label_encoder.classes_)\n",
    "## Start model\n",
    "model = Custom_Model(model = model_alibaba,num_classes = num_labels)\n",
    "model.load_state_dict(torch.load('/content/best_model.pth'))\n",
    "model.to(device)\n",
    "print(\"Model loaded from model.pth\")\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "# model = AutoModel.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "def encode_predict(texts):\n",
    "    encoded = tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "## Custom model \n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,model,num_classes):\n",
    "        super(Custom_Model,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state[:, 0, :]  # BERT embeddings\n",
    "        x = self.relu(self.fc1(last_hidden_state))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc3(x)  # Logits are the raw predictions\n",
    "        return logits  # Ensure that logits are returned\n",
    "\n",
    "def predict(file_path, file_output):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['product_name'] = df['product_name'].apply(normalized)  # Make sure 'normalized' is defined\n",
    "    input_ids, attention_mask = encode_predict(df['product_name'].tolist())\n",
    "\n",
    "    ## Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(input_ids), 16)):\n",
    "            batch_input_ids = input_ids[i:i+16].to(device)\n",
    "            batch_attention_mask = attention_mask[i:i+16].to(device)\n",
    "\n",
    "            outputs = model(batch_input_ids, batch_attention_mask)\n",
    "            batch_scores = torch.softmax(outputs, dim=-1)\n",
    "            batch_predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            # Accumulate predictions and scores\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            scores.extend(batch_scores.max(dim=-1).values.cpu().numpy())\n",
    "\n",
    "    # Ensure the length of predictions and scores matches the length of the DataFrame\n",
    "    if len(predictions) != len(df) or len(scores) != len(df):\n",
    "        raise ValueError(f\"Mismatch between predictions/scores and DataFrame size: {len(predictions)} predictions, {len(df)} rows\")\n",
    "\n",
    "    predicted_label = predictions\n",
    "    scores = scores\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    df['predicted_label'] = label_encoder.inverse_transform(predicted_label)\n",
    "    df['scores'] = scores\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df.to_excel(file_output, index=False)\n",
    "\n",
    "    print(f\"Export done: Data saved at {file_output}\")\n",
    "\n",
    "  \n",
    "\n",
    "predict(r\"/content/mat_na_clean.xlsx\",r\"/content/mat_na_predict.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
